{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop2_MT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j36yKTdPiRx2",
        "colab_type": "text"
      },
      "source": [
        "#AIS MACHINE TRANSLATION WORKSHOP\n",
        "\n",
        "*Using RNN (Recurrent Neural Network) for Natural Language Processing to translate data from French to English.*\n",
        "\n",
        "\n",
        "By Michael Le, Maitreyee Mhasakar\n",
        "\n",
        "Content and other contributions by Janam Parikh, Arshdeep Singh, Rama Narayan Lakshmanan\n",
        "\n",
        "Github link to resources: [https://github.com/aisutd/Fall19_Workshop2_Machine_Translation](https://github.com/aisutd/Fall19_Workshop2_Machine_Translation)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2quj0ay6oDY3",
        "colab_type": "text"
      },
      "source": [
        "## What is Natural Language Processing?\n",
        "\n",
        "Natural Language Processing, usually shortened as NLP,subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
        "\n",
        "The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n",
        "\n",
        "<img src='https://res.cloudinary.com/rsmglobal/image/fetch/t_default%2Cf_auto%2Cq_auto/https://www.rsm.global/singapore/sites/default/files/media/Publications/Our%20Expert%20Insights/rsm-tmt-nlp.jpg' height=\"500\" width=\"600\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## What is Machine Translation?\n",
        "\n",
        "Machine translation (MT) refers to fully automated software that can translate source content into target languages. \n",
        "Humans may use MT to help them render text and speech into another language, or the MT software may operate without human intervention.\n",
        "\n",
        "\n",
        "Main approaches to machine translation:\n",
        "\n",
        "*   **First-generation rule-based (RbMT) systems** : Based on Grammar, Syntax, Phraseology\n",
        "\n",
        "*   **Statistical systems (SMT)** : Based on Search and Big Data.With lots of parallel texts becoming available, SMT developers learned to pattern-match reference texts to find translations that are statistically most likely to be suitable. These systems train faster than RbMT, provided there is enough existing language material to reference.\n",
        " \n",
        "*   **Neural MT (NMT)** : Machine learning technology to teach software how to produce the best result. This process consumes large amounts of processing power, and that is why itâ€™s often run on graphics units of CPUs. NMT started gaining visibility in 2016. Many MT providers are now switching to this technology.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygqOQ5TIiUYy",
        "colab_type": "code",
        "outputId": "dbd27d89-67bd-47c2-f762-652e89ea00d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "#Importing required libraries\n",
        "\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "import io\n",
        "import numpy as np\n",
        "from numpy import array, argmax, random, take\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Input, RepeatVector, TimeDistributed, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model, Model\n",
        "from keras.utils import to_categorical\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW6IDhQBrPc7",
        "colab_type": "code",
        "outputId": "a5e25ae0-7ad0-4540-9968-a896eca339b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NifBc-u6k3HP",
        "colab_type": "code",
        "outputId": "863d2b29-08bb-43b6-bc7a-decb4ea4bbda",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "#Upload dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6a553c93-dbfe-489f-b242-cc5b646e7a39\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6a553c93-dbfe-489f-b242-cc5b646e7a39\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving small_vocab_en to small_vocab_en\n",
            "Saving small_vocab_fr to small_vocab_fr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crSe10OXgPgf",
        "colab_type": "code",
        "outputId": "a50b3785-85ab-4683-9f58-3225c0ae4f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "#Import English sentence data from file into a dataframe\n",
        "english_df = pd.read_csv('small_vocab_en', sep='\\n', header=None, names=['English'])\n",
        "print(english_df.head())\n",
        "english_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             English\n",
            "0  new jersey is sometimes quiet during autumn , ...\n",
            "1  the united states is usually chilly during jul...\n",
            "2  california is usually quiet during march , and...\n",
            "3  the united states is sometimes mild during jun...\n",
            "4  your least liked fruit is the grape , but my l...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(137860, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb0cMxaygsqR",
        "colab_type": "code",
        "outputId": "0f50cf98-0d1a-478c-822b-3a2e3e80d259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "#Import French sentence data from file into a dataframe\n",
        "french_df = pd.read_csv('small_vocab_fr', sep='\\n', header=None, names=['French'])\n",
        "print(french_df.head())\n",
        "french_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              French\n",
            "0  new jersey est parfois calme pendant l' automn...\n",
            "1  les Ã©tats-unis est gÃ©nÃ©ralement froid en juill...\n",
            "2  california est gÃ©nÃ©ralement calme en mars , et...\n",
            "3  les Ã©tats-unis est parfois lÃ©gÃ¨re en juin , et...\n",
            "4  votre moins aimÃ© fruit est le raisin , mais mo...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(137860, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QB07PxmlC7b",
        "colab_type": "code",
        "outputId": "81064802-2b2c-49eb-ab5f-26d61e25608e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "#Final dataset dataframe\n",
        "df = pd.concat([english_df, french_df], axis=1, join='inner')\n",
        "df.info()\n",
        "print(df.head())\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 137860 entries, 0 to 137859\n",
            "Data columns (total 2 columns):\n",
            "English    137860 non-null object\n",
            "French     137860 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 2.1+ MB\n",
            "                                             English                                             French\n",
            "0  new jersey is sometimes quiet during autumn , ...  new jersey est parfois calme pendant l' automn...\n",
            "1  the united states is usually chilly during jul...  les Ã©tats-unis est gÃ©nÃ©ralement froid en juill...\n",
            "2  california is usually quiet during march , and...  california est gÃ©nÃ©ralement calme en mars , et...\n",
            "3  the united states is sometimes mild during jun...  les Ã©tats-unis est parfois lÃ©gÃ¨re en juin , et...\n",
            "4  your least liked fruit is the grape , but my l...  votre moins aimÃ© fruit est le raisin , mais mo...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(137860, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObJLyKHylisA",
        "colab_type": "code",
        "outputId": "1a198529-74cf-404e-8e5b-1e1e3fd04543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Remove missing and blank records from data\n",
        "\"\"\"\n",
        "df['English'].replace('', np.nan, inplace=True)\n",
        "df['French'].replace('', np.nan, inplace=True)\n",
        "df.dropna(subset=['English'], inplace=True)\n",
        "df.dropna(subset=['French'], inplace=True)\n",
        "print(df.shape)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndf['English'].replace('', np.nan, inplace=True)\\ndf['French'].replace('', np.nan, inplace=True)\\ndf.dropna(subset=['English'], inplace=True)\\ndf.dropna(subset=['French'], inplace=True)\\nprint(df.shape)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mBVkzIbl3j8",
        "colab_type": "code",
        "outputId": "4ecd4b50-4e7f-468e-9454-90373fc59bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "#Lowercase english sentences as part of preprocessing\n",
        "df1=df.copy()\n",
        "df1[\"English\"] = df1[\"English\"].str.lower()\n",
        "print(df1.head())\n",
        "print(df1.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             English                                             French\n",
            "0  new jersey is sometimes quiet during autumn , ...  new jersey est parfois calme pendant l' automn...\n",
            "1  the united states is usually chilly during jul...  les Ã©tats-unis est gÃ©nÃ©ralement froid en juill...\n",
            "2  california is usually quiet during march , and...  california est gÃ©nÃ©ralement calme en mars , et...\n",
            "3  the united states is sometimes mild during jun...  les Ã©tats-unis est parfois lÃ©gÃ¨re en juin , et...\n",
            "4  your least liked fruit is the grape , but my l...  votre moins aimÃ© fruit est le raisin , mais mo...\n",
            "(137860, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPY_lYljSrKI",
        "colab_type": "text"
      },
      "source": [
        "## What are Neural Networks?\n",
        "\n",
        "A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. \n",
        "In this sense, neural networks refer to systems of neurons, either organic or artificial in nature. \n",
        "\n",
        "Neural networks can adapt to changing input; so, the network generates the best possible result without needing to redesign the output criteria. \n",
        "\n",
        "The concept of neural networks, which has its roots in artificial intelligence, is swiftly gaining popularity in the development of trading systems.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1592/1*yGMk1GSKKbyKr_cMarlWnA.jpeg'>\n",
        "\n",
        "\n",
        "\n",
        "**Three fundamental components** of neural networks:\n",
        "\n",
        "1. **Structure** - what the neural network looks like, including all the mathematical functions involved, the number of inputs and outputs, and the parameters, called **weights** that the network has to learn.\n",
        "    \n",
        "2. **Loss Function** - a metric that tells us how good or bad the network's predictions are. \n",
        "3. **Optimizer** - the algorithm used for **learning the weights** that give the network the best predictions.\n",
        "\n",
        "\n",
        "### The Simplest Neural Network - The Perceptron\n",
        "The perceptron, arguably the simplest neural network, was invented by psychologist Frank Rosenblatt in 1957 and looks something like this:\n",
        "![perceptron](https://docs.google.com/uc?export=download&id=1SbHK9XPrP1PSO9T-lh9uG9CTCNjdXhU1)\n",
        "\n",
        "(image source: http://ataspinar.com/2016/12/22/the-perceptron/)\n",
        "\n",
        "A perceptron is basically a neural network with a single **artificial neuron**. Similar to the biological neuron, a perceptron has the following characteristics:\n",
        "\n",
        "- **inputs** - the perceptron receives a given number of real-valued inputs (the inputs are numbers).\n",
        "- **weights** - the perceptron has a weight $ w_i $ associated with each input $ x_i $. These weighted connections are like synapses and they are parameters that the perceptron must \"learn\".\n",
        "- **weighted sum (basically a dot product)** - the inputs are multiplied by the weights and the results are added together to produce a weighted sum.\n",
        "- **activation function** - the perceptron has an activation function called the unit-step function that produces an output of 1 if the weighted sum is greater than some threshold $\\theta$ and -1 otherwise.\n",
        "  \n",
        "\n",
        "*tanh*:\n",
        "tanh is like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n",
        "\n",
        "![tanh](https://miro.medium.com/max/744/1*f9erByySVjTjohfFdNkJYQ.jpeg)\n",
        "\n",
        "*Softmax*: \n",
        "Softmax function takes a vector as input and produces a vector of the same shape as the output. In a way, this function basically acts on an entire layer. The softmax function basically converts a vector of real values into a probability distribution and is useful for representing the probabilities of different classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Hidden layers** : layer of neurons other than the input and output layers\n",
        "\n",
        "**Dropout** : Technique to reduce overfitting in neural networks by shutting particular or random neurons at a point of time.\n",
        "\n",
        "**Loss function** :  Method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction.\n",
        "\n",
        "*Cross-entropy loss*: measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
        "\n",
        "\n",
        "**Forward Pass**: The forward pass refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer.\n",
        "\n",
        "**Backpropagation**:\n",
        "Backward pass refers to process of counting changes in weights, using gradient descent algorithm (or similar). Computation is made from last layer, backward to the first layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFQEOXbsqr0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization\n",
        "def tokenization(sentences):\n",
        "      tokenizer = Tokenizer(lower=False)\n",
        "      tokenizer.fit_on_texts(sentences)\n",
        "      return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp1-UCZgqxtd",
        "colab_type": "code",
        "outputId": "2374c7b4-4524-4ced-edea-8a707c1d3f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#English Tokenization and Unique word/Vocabulary count\n",
        "eng_tokenizer = tokenization(df1[\"English\"].astype('str'))\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTKrhbgNucNN",
        "colab_type": "code",
        "outputId": "5d4efc97-d4e4-4309-eec1-01283281eb8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#French Tokenization and Unique word/Vocabulary count\n",
        "\n",
        "fren_tokenizer = tokenization(df1[\"French\"].astype('str'))\n",
        "\n",
        "print(f'French Vocabulary Size: {len(fren_tokenizer.word_index) + 1}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "French Vocabulary Size: 346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-tbQ_nBvUAv",
        "colab_type": "code",
        "outputId": "756bca38-4522-477d-9cc4-00c85193eb30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#Convert text to integer sequences for English\n",
        "english_sequences = eng_tokenizer.texts_to_sequences(df1[\"English\"].values)\n",
        "print(english_sequences[0])\n",
        "print(df1[\"English\"].values[0])\n",
        "print(eng_tokenizer.word_index)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[17, 23, 1, 8, 67, 4, 39, 7, 3, 1, 55, 2, 44]\n",
            "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "{'is': 1, 'in': 2, 'it': 3, 'during': 4, 'the': 5, 'but': 6, 'and': 7, 'sometimes': 8, 'usually': 9, 'never': 10, 'favorite': 11, 'least': 12, 'fruit': 13, 'most': 14, 'loved': 15, 'liked': 16, 'new': 17, 'paris': 18, 'india': 19, 'united': 20, 'states': 21, 'california': 22, 'jersey': 23, 'france': 24, 'china': 25, 'he': 26, 'she': 27, 'grapefruit': 28, 'your': 29, 'my': 30, 'his': 31, 'her': 32, 'fall': 33, 'june': 34, 'spring': 35, 'january': 36, 'winter': 37, 'march': 38, 'autumn': 39, 'may': 40, 'nice': 41, 'september': 42, 'july': 43, 'april': 44, 'november': 45, 'summer': 46, 'december': 47, 'february': 48, 'our': 49, 'their': 50, 'freezing': 51, 'pleasant': 52, 'beautiful': 53, 'october': 54, 'snowy': 55, 'warm': 56, 'cold': 57, 'wonderful': 58, 'dry': 59, 'busy': 60, 'august': 61, 'chilly': 62, 'rainy': 63, 'mild': 64, 'wet': 65, 'relaxing': 66, 'quiet': 67, 'hot': 68, 'dislikes': 69, 'likes': 70, 'limes': 71, 'lemons': 72, 'grapes': 73, 'mangoes': 74, 'apples': 75, 'peaches': 76, 'oranges': 77, 'pears': 78, 'strawberries': 79, 'bananas': 80, 'to': 81, 'grape': 82, 'apple': 83, 'orange': 84, 'lemon': 85, 'lime': 86, 'banana': 87, 'mango': 88, 'pear': 89, 'strawberry': 90, 'peach': 91, 'like': 92, 'dislike': 93, 'they': 94, 'that': 95, 'i': 96, 'we': 97, 'you': 98, 'animal': 99, 'a': 100, 'truck': 101, 'car': 102, 'automobile': 103, 'was': 104, 'next': 105, 'go': 106, 'driving': 107, 'visit': 108, 'little': 109, 'big': 110, 'old': 111, 'yellow': 112, 'red': 113, 'rusty': 114, 'blue': 115, 'white': 116, 'black': 117, 'green': 118, 'shiny': 119, 'are': 120, 'last': 121, 'feared': 122, 'animals': 123, 'this': 124, 'plan': 125, 'going': 126, 'saw': 127, 'disliked': 128, 'drives': 129, 'drove': 130, 'between': 131, 'translate': 132, 'plans': 133, 'were': 134, 'went': 135, 'might': 136, 'wanted': 137, 'thinks': 138, 'spanish': 139, 'portuguese': 140, 'chinese': 141, 'english': 142, 'french': 143, 'translating': 144, 'difficult': 145, 'fun': 146, 'easy': 147, 'wants': 148, 'think': 149, 'why': 150, \"it's\": 151, 'did': 152, 'cat': 153, 'shark': 154, 'bird': 155, 'mouse': 156, 'horse': 157, 'elephant': 158, 'dog': 159, 'monkey': 160, 'lion': 161, 'bear': 162, 'rabbit': 163, 'snake': 164, 'when': 165, 'want': 166, 'do': 167, 'how': 168, 'elephants': 169, 'horses': 170, 'dogs': 171, 'sharks': 172, 'snakes': 173, 'cats': 174, 'rabbits': 175, 'monkeys': 176, 'bears': 177, 'birds': 178, 'lions': 179, 'mice': 180, \"didn't\": 181, 'eiffel': 182, 'tower': 183, 'grocery': 184, 'store': 185, 'football': 186, 'field': 187, 'lake': 188, 'school': 189, 'would': 190, \"aren't\": 191, 'been': 192, 'weather': 193, 'does': 194, 'has': 195, \"isn't\": 196, 'am': 197, 'where': 198, 'have': 199}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5HGMsbkvvMO",
        "colab_type": "code",
        "outputId": "89f75494-ac08-41b3-d3ab-03e18b65eec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#Convert text to integer sequences for French\n",
        "\n",
        "french_sequences = fren_tokenizer.texts_to_sequences(df1[\"French\"].values)\n",
        "print(french_sequences[0])\n",
        "print(df1[\"French\"].values[0])\n",
        "print(fren_tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[35, 34, 1, 8, 67, 37, 11, 24, 6, 3, 1, 112, 2, 50]\n",
            "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "{'est': 1, 'en': 2, 'il': 3, 'les': 4, 'mais': 5, 'et': 6, 'la': 7, 'parfois': 8, 'jamais': 9, 'le': 10, \"l'\": 11, 'gÃ©nÃ©ralement': 12, 'moins': 13, 'aimÃ©': 14, 'au': 15, 'fruit': 16, 'prÃ©fÃ©rÃ©': 17, 'agrÃ©able': 18, 'froid': 19, 'son': 20, 'chaud': 21, 'de': 22, 'plus': 23, 'automne': 24, 'mois': 25, 'Ã ': 26, 'elle': 27, 'citrons': 28, 'paris': 29, 'inde': 30, 'unis': 31, 'Ã©tats': 32, 'france': 33, 'jersey': 34, 'new': 35, 'chine': 36, 'pendant': 37, 'pamplemousse': 38, 'mon': 39, 'votre': 40, 'juin': 41, 'printemps': 42, 'janvier': 43, 'hiver': 44, 'mars': 45, 'Ã©tÃ©': 46, 'mai': 47, 'septembre': 48, 'juillet': 49, 'avril': 50, 'novembre': 51, 'dÃ©cembre': 52, 'fÃ©vrier': 53, 'octobre': 54, 'aime': 55, 'aoÃ»t': 56, 'merveilleux': 57, 'relaxant': 58, 'doux': 59, 'humide': 60, 'notre': 61, 'californie': 62, 'sec': 63, 'leur': 64, 'occupÃ©': 65, 'pluvieux': 66, 'calme': 67, 'beau': 68, 'habituellement': 69, 'pommes': 70, 'pÃªches': 71, 'oranges': 72, 'poires': 73, 'fraises': 74, 'bananes': 75, 'verts': 76, 'raisins': 77, 'mangues': 78, \"d'\": 79, 'mangue': 80, 'gel': 81, 'raisin': 82, 'pomme': 83, \"l'orange\": 84, 'citron': 85, 'chaux': 86, 'banane': 87, 'poire': 88, 'fraise': 89, 'pÃªche': 90, 'pas': 91, 'enneigÃ©e': 92, 'favori': 93, 'dÃ©teste': 94, 'gÃ¨le': 95, 'fruits': 96, 'voiture': 97, \"l'automne\": 98, 'ils': 99, \"n'aime\": 100, 'california': 101, 'neige': 102, 'fait': 103, 'belle': 104, 'ne': 105, 'vous': 106, 'nous': 107, 'des': 108, 'animal': 109, 'camion': 110, 'cours': 111, 'neigeux': 112, 'conduit': 113, 'prochain': 114, 'ce': 115, 'je': 116, 'tranquille': 117, 'a': 118, 'cher': 119, 'une': 120, 'cette': 121, 'Ã©tait': 122, 'aller': 123, 'aiment': 124, 'chaude': 125, 'aimons': 126, \"n'aiment\": 127, \"n'aimez\": 128, 'leurs': 129, 'aimez': 130, 'sont': 131, 'dÃ©testons': 132, 'jaune': 133, 'rouge': 134, \"j'aime\": 135, 'visiter': 136, 'sÃ¨che': 137, 'occupÃ©e': 138, 'frisquet': 139, 'prÃ©fÃ©rÃ©e': 140, 'animaux': 141, 'dernier': 142, 'aimait': 143, 'un': 144, 'conduisait': 145, 'que': 146, 'nouvelle': 147, 'vieille': 148, 'vu': 149, 'verte': 150, 'petite': 151, 'nos': 152, 'noire': 153, 'brillant': 154, 'blanche': 155, 'redoutÃ©': 156, 'pleut': 157, \"n'aimait\": 158, 'pamplemousses': 159, 'pense': 160, 'entre': 161, 'bleue': 162, 'nouveau': 163, 'traduire': 164, 'rouillÃ©e': 165, 'bleu': 166, 'se': 167, 'grande': 168, 'rouillÃ©': 169, 'ses': 170, \"qu'il\": 171, 'blanc': 172, 'aux': 173, 'brillante': 174, 'prÃ©fÃ©rÃ©s': 175, 'noir': 176, 'pluies': 177, 'envisage': 178, 'Ã©taient': 179, 'va': 180, 'rendre': 181, 'vert': 182, 'vieux': 183, 'petit': 184, 'espagnol': 185, 'portugais': 186, 'chinois': 187, 'anglais': 188, 'franÃ§ais': 189, 'glaciales': 190, 'mes': 191, 'cet': 192, 'automobile': 193, 'traduction': 194, 'mouillÃ©': 195, 'difficile': 196, 'amusant': 197, 'facile': 198, 'comme': 199, 'gros': 200, 'souris': 201, 'pourrait': 202, 'voulait': 203, 'veut': 204, 'pourquoi': 205, 'aimÃ©s': 206, 'prÃ©vois': 207, 'prÃ©voyons': 208, 'vos': 209, 'intention': 210, 'clÃ©mentes': 211, 'ont': 212, 'chat': 213, 'requin': 214, 'cheval': 215, 'chien': 216, 'singe': 217, 'lion': 218, 'ours': 219, 'lapin': 220, 'serpent': 221, 'redoutÃ©s': 222, 'allÃ©': 223, 'grosse': 224, 'pluie': 225, 'trop': 226, 'monde': 227, 'maillot': 228, 'vont': 229, 'volant': 230, 'avez': 231, 'i': 232, 'allÃ©s': 233, 'allÃ©e': 234, 'quand': 235, 'oiseau': 236, 'Ã©lÃ©phant': 237, 'pourraient': 238, 'voulaient': 239, 'veulent': 240, 'dÃ©tendre': 241, 'aimÃ©e': 242, 'magnifique': 243, \"l'automobile\": 244, \"n'aimons\": 245, 'gelÃ©': 246, 'dÃ©testait': 247, 'grand': 248, 'bien': 249, 'vers': 250, 'prÃ©voient': 251, 'prÃ©voit': 252, 'lui': 253, 'visite': 254, 'comment': 255, 'Ã©lÃ©phants': 256, 'chevaux': 257, 'chiens': 258, \"l'Ã©lÃ©phant\": 259, \"l'oiseau\": 260, 'requins': 261, \"l'ours\": 262, 'serpents': 263, 'chats': 264, 'lapins': 265, 'singes': 266, 'oiseaux': 267, 'lions': 268, 'lÃ©gÃ¨re': 269, 'cÃ©page': 270, 'pensez': 271, 'Ã‰tats': 272, 'tour': 273, 'eiffel': 274, \"l'Ã©picerie\": 275, 'terrain': 276, 'football': 277, 'lac': 278, \"l'Ã©cole\": 279, \"l'animal\": 280, \"n'est\": 281, 'allons': 282, 'allez': 283, 'peu': 284, 'pousse': 285, 'du': 286, 'temps': 287, 'at': 288, 'rouille': 289, 'sur': 290, \"qu'elle\": 291, 'petites': 292, 'derniÃ¨re': 293, 'Ãªtes': 294, 'vais': 295, 'voudrait': 296, 'proches': 297, 'frais': 298, 'manguiers': 299, 'avons': 300, 't': 301, 'porcelaine': 302, 'dÃ©testez': 303, \"c'est\": 304, 'grandes': 305, 'prÃ©fÃ©rÃ©es': 306, 'douce': 307, 'durant': 308, 'congÃ©lation': 309, 'plaÃ®t': 310, 'oÃ¹': 311, 'dans': 312, 'voulez': 313, 'aimeraient': 314, \"n'a\": 315, 'petits': 316, 'grands': 317, 'limes': 318, 'envisagent': 319, 'grosses': 320, 'bÃ©nigne': 321, 'mouillÃ©e': 322, 'enneigÃ©': 323, 'moindres': 324, 'conduite': 325, 'gelÃ©s': 326, 'tout': 327, 'etats': 328, \"n'Ãªtes\": 329, 'vit': 330, 'ressort': 331, 'dÃ©tend': 332, 'redoutÃ©e': 333, 'tu': 334, 'qui': 335, 'traduis': 336, 'apprÃ©ciÃ©': 337, 'allions': 338, 'trouvÃ©': 339, 'as': 340, 'faire': 341, 'favoris': 342, 'souvent': 343, 'es': 344, 'moteur': 345}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFsjIJSEyFyd",
        "colab_type": "code",
        "outputId": "2ed25ab1-6da5-4776-d13e-c14bb0922e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "#Pad sequences with zeros to amke them of equal length for processing\n",
        "english_sequences = pad_sequences(english_sequences, padding='post')\n",
        "french_sequences = pad_sequences(french_sequences, padding='post')\n",
        "print(english_sequences.shape)\n",
        "print(english_sequences[0])\n",
        "print(french_sequences.shape)\n",
        "print(french_sequences[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(137860, 15)\n",
            "[17 23  1  8 67  4 39  7  3  1 55  2 44  0  0]\n",
            "(137860, 21)\n",
            "[ 35  34   1   8  67  37  11  24   6   3   1 112   2  50   0   0   0   0\n",
            "   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a68idwNXwMDg",
        "colab_type": "code",
        "outputId": "3d078409-33fa-46e0-f573-cedc1e382d3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "#Split data into train and test data\n",
        "train_french_input, test_french_input, train_english_output, test_english_output = train_test_split(french_sequences, \n",
        "                                                    english_sequences, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=1)\n",
        "\n",
        "num_train_samples = train_french_input.shape[0]\n",
        "num_test_samples = test_french_input.shape[0]\n",
        "print(f'Number of training samples: {num_train_samples}')\n",
        "print(f'Number of testing samples:  {num_test_samples}')\n",
        "print()\n",
        "\n",
        "max_english_sentence_length = train_french_input.shape[1]\n",
        "max_french_sentence_length = train_french_input.shape[1]\n",
        "print(f'Max english sentence length:    {max_english_sentence_length}')\n",
        "print(f'Max french sentence length:     {max_french_sentence_length}')\n",
        "print()\n",
        "\n",
        "train_french_input = train_french_input.reshape(num_train_samples, max_french_sentence_length, 1)\n",
        "train_english_output = pad_sequences(train_english_output, maxlen=max_french_sentence_length, padding='post')\n",
        "train_english_output = train_english_output.reshape(num_train_samples, max_french_sentence_length, 1)\n",
        "\n",
        "test_french_input = test_french_input.reshape(num_test_samples, max_french_sentence_length, 1)\n",
        "test_english_output = pad_sequences(test_english_output, maxlen=max_french_sentence_length, padding='post')\n",
        "test_english_output = test_english_output.reshape(num_test_samples, max_french_sentence_length, 1)\n",
        "\n",
        "print(f'Train French:   {train_french_input.shape}')\n",
        "print(f'Test French:    {test_french_input.shape}')\n",
        "print(f'Train English:  {train_english_output.shape}')\n",
        "print(f'Test English:   {test_english_output.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 110288\n",
            "Number of testing samples:  27572\n",
            "\n",
            "Max english sentence length:    21\n",
            "Max french sentence length:     21\n",
            "\n",
            "Train French:   (110288, 21, 1)\n",
            "Test French:    (27572, 21, 1)\n",
            "Train English:  (110288, 21, 1)\n",
            "Test English:   (27572, 21, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbqlYKQbTG3C",
        "colab_type": "text"
      },
      "source": [
        "## What are Recurrent Neural Networks?\n",
        "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed graph along a sequence. This allows it to exhibit dynamic temporal behavior for a time sequence. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
        "\n",
        "\n",
        "RNNs are designed to take sequences of text as inputs or return sequences of text as outputs, or both. \n",
        "\n",
        "They're called recurrent because the network's hidden layers have a loop in which the output from one time step becomes an input at the next time step. This recurrence serves as a form of memory. \n",
        "\n",
        "It allows contextual information to flow through the network so that relevant outputs from previous time steps can be applied to network operations at the current time step. \n",
        "\n",
        "<img src=\"https://qph.fs.quoracdn.net/main-qimg-6eced51767f5bcd94b32bbe50da438e9\">\n",
        "\n",
        "# **Vanishing Gradient Problem **\n",
        "\n",
        "As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train.\n",
        "\n",
        "A large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small.\n",
        "\n",
        "A small gradient means that the weights and biases of the initial layers will not be updated effectively with each training session. Since these initial layers are often crucial to recognizing the core elements of the input data, it can lead to overall inaccuracy of the whole network.\n",
        "\n",
        "\n",
        "\n",
        "## What are LSTMs (Long short-term memory)?\n",
        "\n",
        "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
        "\n",
        "\n",
        "A common LSTM unit is composed of a **cell**, an **input gate**, an **output gate** and a **forget gate**. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
        "\n",
        "RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged.\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png'>\n",
        "\n",
        "\n",
        "\n",
        "**The cell** : Responsible for keeping track of the dependencies between the elements in the input sequence. \n",
        "\n",
        "**The input gate** : Controls the extent to which a new value flows into the cell.\n",
        "\n",
        "**The forget gate**: Controls the extent to which a value remains in the cell and the output gate controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit. \n",
        "\n",
        "The activation function of the LSTM gates is often the logistic sigmoid function.\n",
        "\n",
        "<img src='https://miro.medium.com/max/2840/1*0f8r3Vd-i4ueYND1CUrhMA.png'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfzeifs8xgRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "#Create and Build the RNN model\n",
        "model = Sequential()\n",
        "#model.add(Embedding(input_dim=len(fren_tokenizer.word_index) + 1, output_dim=128, mask_zero=True))\n",
        "\n",
        "\n",
        "# return sequences is to get the output of the LSTM for each time step to pass\n",
        "#   to the next layer in the model\n",
        "model.add(LSTM(256, input_shape=train_french_input.shape[1:], return_sequences=True)) # Layer 1 (Input Layer)\n",
        "\n",
        "model.add(TimeDistributed(Dense(512, activation='tanh'))) # Layer 2 (Only hidden layer)\n",
        "\n",
        "# model ouput probabilities for english words from input word\n",
        "model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax'))) # Final (Output) Layer\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12pnwzua0QLl",
        "colab_type": "code",
        "outputId": "0578f293-b0b1-494f-f698-0f1971ecb14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Run the model on training data\n",
        "\n",
        "model.fit(train_french_input, train_english_output, batch_size=1024, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "110288/110288 [==============================] - 12s 105us/step - loss: 0.4844 - acc: 0.8223\n",
            "Epoch 2/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4747 - acc: 0.8236\n",
            "Epoch 3/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4660 - acc: 0.8252\n",
            "Epoch 4/50\n",
            "110288/110288 [==============================] - 12s 104us/step - loss: 0.4575 - acc: 0.8274\n",
            "Epoch 5/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4513 - acc: 0.8280\n",
            "Epoch 6/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4445 - acc: 0.8299\n",
            "Epoch 7/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4389 - acc: 0.8323\n",
            "Epoch 8/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4340 - acc: 0.8319\n",
            "Epoch 9/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4292 - acc: 0.8327\n",
            "Epoch 10/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4241 - acc: 0.8345\n",
            "Epoch 11/50\n",
            "110288/110288 [==============================] - 12s 105us/step - loss: 0.4210 - acc: 0.8351\n",
            "Epoch 12/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4175 - acc: 0.8367\n",
            "Epoch 13/50\n",
            "110288/110288 [==============================] - 12s 105us/step - loss: 0.4138 - acc: 0.8371\n",
            "Epoch 14/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4106 - acc: 0.8378\n",
            "Epoch 15/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4077 - acc: 0.8375\n",
            "Epoch 16/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4057 - acc: 0.8400\n",
            "Epoch 17/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.4024 - acc: 0.8398\n",
            "Epoch 18/50\n",
            "110288/110288 [==============================] - 12s 106us/step - loss: 0.4009 - acc: 0.8412\n",
            "Epoch 19/50\n",
            "110288/110288 [==============================] - 12s 104us/step - loss: 0.3981 - acc: 0.8417\n",
            "Epoch 20/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3969 - acc: 0.8421\n",
            "Epoch 21/50\n",
            "110288/110288 [==============================] - 11s 103us/step - loss: 0.3943 - acc: 0.8427\n",
            "Epoch 22/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3925 - acc: 0.8427\n",
            "Epoch 23/50\n",
            "110288/110288 [==============================] - 12s 104us/step - loss: 0.3910 - acc: 0.8432\n",
            "Epoch 24/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3896 - acc: 0.8444\n",
            "Epoch 25/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3882 - acc: 0.8452\n",
            "Epoch 26/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3865 - acc: 0.8452\n",
            "Epoch 27/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3851 - acc: 0.8457\n",
            "Epoch 28/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3837 - acc: 0.8456\n",
            "Epoch 29/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3820 - acc: 0.8461\n",
            "Epoch 30/50\n",
            "110288/110288 [==============================] - 12s 104us/step - loss: 0.3804 - acc: 0.8467\n",
            "Epoch 31/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3797 - acc: 0.8472\n",
            "Epoch 32/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3779 - acc: 0.8481\n",
            "Epoch 33/50\n",
            "110288/110288 [==============================] - 11s 103us/step - loss: 0.3770 - acc: 0.8484\n",
            "Epoch 34/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3754 - acc: 0.8494\n",
            "Epoch 35/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3754 - acc: 0.8491\n",
            "Epoch 36/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3733 - acc: 0.8488\n",
            "Epoch 37/50\n",
            "110288/110288 [==============================] - 12s 104us/step - loss: 0.3721 - acc: 0.8501\n",
            "Epoch 38/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3723 - acc: 0.8495\n",
            "Epoch 39/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3706 - acc: 0.8508\n",
            "Epoch 40/50\n",
            "110288/110288 [==============================] - 12s 105us/step - loss: 0.3704 - acc: 0.8497\n",
            "Epoch 41/50\n",
            "110288/110288 [==============================] - 12s 104us/step - loss: 0.3683 - acc: 0.8522\n",
            "Epoch 42/50\n",
            "110288/110288 [==============================] - 12s 105us/step - loss: 0.3673 - acc: 0.8518\n",
            "Epoch 43/50\n",
            "110288/110288 [==============================] - 12s 104us/step - loss: 0.3673 - acc: 0.8510\n",
            "Epoch 44/50\n",
            "110288/110288 [==============================] - 12s 105us/step - loss: 0.3661 - acc: 0.8518\n",
            "Epoch 45/50\n",
            "110288/110288 [==============================] - 12s 106us/step - loss: 0.3654 - acc: 0.8515\n",
            "Epoch 46/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3641 - acc: 0.8539\n",
            "Epoch 47/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3639 - acc: 0.8525\n",
            "Epoch 48/50\n",
            "110288/110288 [==============================] - 11s 104us/step - loss: 0.3623 - acc: 0.8536\n",
            "Epoch 49/50\n",
            "110288/110288 [==============================] - 12s 105us/step - loss: 0.3624 - acc: 0.8540\n",
            "Epoch 50/50\n",
            "110288/110288 [==============================] - 12s 105us/step - loss: 0.3611 - acc: 0.8539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7c3bb836a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3tl93igTzCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('50_epochs_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV5tlM7O_ITs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load data\n",
        "model = load_model('50_epochs_model.h5')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlELtfDwoX4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict on unseen data\n",
        "sen_prediction = model.predict_classes(test_french_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAAkxhlunQ_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "          if index == n:\n",
        "              return word\n",
        "      return None\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePvVwom4n8TI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_text=[]\n",
        "for i in sen_prediction:\n",
        "    temp = []\n",
        "    for j in range(len(i)):\n",
        "      t = get_word(i[j], eng_tokenizer)\n",
        "      if j > 0:\n",
        "        if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "          temp.append('')\n",
        "        else:\n",
        "          temp.append(t)\n",
        "      else:\n",
        "        if(t == None):\n",
        "          temp.append('')\n",
        "        else:\n",
        "          temp.append(t)\n",
        "\n",
        "    preds_text.append(' '.join(temp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-ndicaCJJQ",
        "colab_type": "code",
        "outputId": "f9a6d6f0-8a9b-4086-f865-6a8b235e6e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#Original Output and Predictions \n",
        "\n",
        "print(f\"Original Sentence:     {' '.join(fren_tokenizer.sequences_to_texts(test_french_input[50]))}\")\n",
        "\n",
        "\n",
        "print(f\"Expected Sentence:     {' '.join(eng_tokenizer.sequences_to_texts(test_english_output[50]))}\")\n",
        "\n",
        "print(\"Predicted Sentence:   \",preds_text[50])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Sentence:     paris est gÃ©nÃ©ralement agrÃ©able en mars et il est merveilleux Ã  l' automne        \n",
            "Expected Sentence:     paris is usually nice during march and it is wonderful in autumn         \n",
            "Predicted Sentence:    paris is usually cold during march and it is usually in fall         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkiupTQc40tx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "88131dc2-acb8-4b84-8049-af7a3403b56c"
      },
      "source": [
        "\n",
        "print(fren_tokenizer.sequences_to_texts(test_french_input[2]))\n",
        "print(eng_tokenizer.sequences_to_texts(test_english_output[2]))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"l'\", 'inde', 'est', 'doux', 'Ã ', \"l'automne\", 'mais', 'il', 'est', 'jamais', 'chaud', 'au', 'printemps', '', '', '', '', '', '', '', '']\n",
            "['india', 'is', 'mild', 'during', 'fall', 'but', 'it', 'is', 'never', 'warm', 'in', 'spring', '', '', '', '', '', '', '', '', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcdFQMBZ4rXF",
        "colab_type": "code",
        "outputId": "082f6a0c-be4b-4fff-edf0-c430cb8a4ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(eng_tokenizer.word_index['autumn'])\n",
        "print(eng_tokenizer.word_index['fall'])\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39\n",
            "33\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}