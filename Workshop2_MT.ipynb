{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop2_MT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j36yKTdPiRx2",
        "colab_type": "text"
      },
      "source": [
        "#AIS MACHINE TRANSLATION WORKSHOP\n",
        "\n",
        "*Using RNN (Recurrent Neural Network) for Natural Language Processing to translate data from French to English.*\n",
        "\n",
        "\n",
        "By Michael Le, Maitreyee Mhasakar\n",
        "\n",
        "Content and other contributions by Janam Parikh, Arshdeep Singh, Rama Narayan Lakshmanan\n",
        "\n",
        "Github link to resources: [https://github.com/aisutd/Fall19_Workshop2_Machine_Translation](https://github.com/aisutd/Fall19_Workshop2_Machine_Translation)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2quj0ay6oDY3",
        "colab_type": "text"
      },
      "source": [
        "## What is Natural Language Processing?\n",
        "\n",
        "Natural Language Processing, usually shortened as NLP,subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
        "\n",
        "The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n",
        "\n",
        "<img src='https://res.cloudinary.com/rsmglobal/image/fetch/t_default%2Cf_auto%2Cq_auto/https://www.rsm.global/singapore/sites/default/files/media/Publications/Our%20Expert%20Insights/rsm-tmt-nlp.jpg' height=\"500\" width=\"600\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## What is Machine Translation?\n",
        "\n",
        "Machine translation (MT) refers to fully automated software that can translate source content into target languages. \n",
        "Humans may use MT to help them render text and speech into another language, or the MT software may operate without human intervention.\n",
        "\n",
        "\n",
        "Main approaches to machine translation:\n",
        "\n",
        "*   **First-generation rule-based (RbMT) systems** : Based on Grammar, Syntax, Phraseology\n",
        "\n",
        "*   **Statistical systems (SMT)** : Based on Search and Big Data.With lots of parallel texts becoming available, SMT developers learned to pattern-match reference texts to find translations that are statistically most likely to be suitable. These systems train faster than RbMT, provided there is enough existing language material to reference.\n",
        " \n",
        "*   **Neural MT (NMT)** : Machine learning technology to teach software how to produce the best result. This process consumes large amounts of processing power, and that is why itâ€™s often run on graphics units of CPUs. NMT started gaining visibility in 2016. Many MT providers are now switching to this technology.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygqOQ5TIiUYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing required libraries\n",
        "\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "import io\n",
        "import numpy as np\n",
        "from numpy import array, argmax, random, take\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Input, RepeatVector, TimeDistributed, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model, Model\n",
        "from keras.utils import to_categorical\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW6IDhQBrPc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NifBc-u6k3HP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Upload dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crSe10OXgPgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import English sentence data from file into a dataframe\n",
        "english_df = pd.read_csv('small_vocab_en', sep='\\n', header=None, names=['English'])\n",
        "print(english_df.head())\n",
        "english_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb0cMxaygsqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import French sentence data from file into a dataframe\n",
        "french_df = pd.read_csv('small_vocab_fr', sep='\\n', header=None, names=['French'])\n",
        "print(french_df.head())\n",
        "french_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QB07PxmlC7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Final dataset dataframe\n",
        "df = pd.concat([english_df, french_df], axis=1, join='inner')\n",
        "df.info()\n",
        "print(df.head())\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObJLyKHylisA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove missing and blank records from data\n",
        "\"\"\"\n",
        "df['English'].replace('', np.nan, inplace=True)\n",
        "df['French'].replace('', np.nan, inplace=True)\n",
        "df.dropna(subset=['English'], inplace=True)\n",
        "df.dropna(subset=['French'], inplace=True)\n",
        "print(df.shape)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mBVkzIbl3j8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lowercase english sentences as part of preprocessing\n",
        "df1=df.copy()\n",
        "df1[\"English\"] = df1[\"English\"].str.lower()\n",
        "print(df1.head())\n",
        "print(df1.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPY_lYljSrKI",
        "colab_type": "text"
      },
      "source": [
        "## What are Neural Networks?\n",
        "\n",
        "A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. \n",
        "In this sense, neural networks refer to systems of neurons, either organic or artificial in nature. \n",
        "\n",
        "Neural networks can adapt to changing input; so, the network generates the best possible result without needing to redesign the output criteria. \n",
        "\n",
        "The concept of neural networks, which has its roots in artificial intelligence, is swiftly gaining popularity in the development of trading systems.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1592/1*yGMk1GSKKbyKr_cMarlWnA.jpeg'>\n",
        "\n",
        "\n",
        "\n",
        "**Three fundamental components** of neural networks:\n",
        "\n",
        "1. **Structure** - what the neural network looks like, including all the mathematical functions involved, the number of inputs and outputs, and the parameters, called **weights** that the network has to learn.\n",
        "    \n",
        "2. **Loss Function** - a metric that tells us how good or bad the network's predictions are. \n",
        "3. **Optimizer** - the algorithm used for **learning the weights** that give the network the best predictions.\n",
        "\n",
        "\n",
        "### The Simplest Neural Network - The Perceptron\n",
        "The perceptron, arguably the simplest neural network, was invented by psychologist Frank Rosenblatt in 1957 and looks something like this:\n",
        "![perceptron](https://docs.google.com/uc?export=download&id=1SbHK9XPrP1PSO9T-lh9uG9CTCNjdXhU1)\n",
        "\n",
        "(image source: http://ataspinar.com/2016/12/22/the-perceptron/)\n",
        "\n",
        "A perceptron is basically a neural network with a single **artificial neuron**. Similar to the biological neuron, a perceptron has the following characteristics:\n",
        "\n",
        "- **inputs** - the perceptron receives a given number of real-valued inputs (the inputs are numbers).\n",
        "- **weights** - the perceptron has a weight $ w_i $ associated with each input $ x_i $. These weighted connections are like synapses and they are parameters that the perceptron must \"learn\".\n",
        "- **weighted sum (basically a dot product)** - the inputs are multiplied by the weights and the results are added together to produce a weighted sum.\n",
        "- **activation function** - the perceptron has an activation function called the unit-step function that produces an output of 1 if the weighted sum is greater than some threshold $\\theta$ and -1 otherwise.\n",
        "  \n",
        "\n",
        "*tanh*:\n",
        "tanh is like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n",
        "\n",
        "![tanh](https://miro.medium.com/max/744/1*f9erByySVjTjohfFdNkJYQ.jpeg)\n",
        "\n",
        "*Softmax*: \n",
        "Softmax function takes a vector as input and produces a vector of the same shape as the output. In a way, this function basically acts on an entire layer. The softmax function basically converts a vector of real values into a probability distribution and is useful for representing the probabilities of different classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Hidden layers** : layer of neurons other than the input and output layers\n",
        "\n",
        "**Dropout** : Technique to reduce overfitting in neural networks by shutting particular or random neurons at a point of time.\n",
        "\n",
        "**Loss function** :  Method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction.\n",
        "\n",
        "*Cross-entropy loss*: measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
        "\n",
        "\n",
        "**Forward Pass**: The forward pass refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer.\n",
        "\n",
        "**Backpropagation**:\n",
        "Backward pass refers to process of counting changes in weights, using gradient descent algorithm (or similar). Computation is made from last layer, backward to the first layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFQEOXbsqr0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization\n",
        "def tokenization(sentences):\n",
        "      tokenizer = Tokenizer(lower=False)\n",
        "      tokenizer.fit_on_texts(sentences)\n",
        "      return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp1-UCZgqxtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#English Tokenization and Unique word/Vocabulary count\n",
        "eng_tokenizer = tokenization(df1[\"English\"].astype('str'))\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTKrhbgNucNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#French Tokenization and Unique word/Vocabulary count\n",
        "\n",
        "fren_tokenizer = tokenization(df1[\"French\"].astype('str'))\n",
        "\n",
        "print(f'French Vocabulary Size: {len(fren_tokenizer.word_index) + 1}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-tbQ_nBvUAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert text to integer sequences for English\n",
        "english_sequences = eng_tokenizer.texts_to_sequences(df1[\"English\"].values)\n",
        "print(english_sequences[0])\n",
        "print(df1[\"English\"].values[0])\n",
        "print(eng_tokenizer.word_index)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5HGMsbkvvMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert text to integer sequences for French\n",
        "\n",
        "french_sequences = fren_tokenizer.texts_to_sequences(df1[\"French\"].values)\n",
        "print(french_sequences[0])\n",
        "print(df1[\"French\"].values[0])\n",
        "print(fren_tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFsjIJSEyFyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Pad sequences with zeros to amke them of equal length for processing\n",
        "english_sequences = pad_sequences(english_sequences, padding='post')\n",
        "french_sequences = pad_sequences(french_sequences, padding='post')\n",
        "print(english_sequences.shape)\n",
        "print(english_sequences[0])\n",
        "print(french_sequences.shape)\n",
        "print(french_sequences[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a68idwNXwMDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split data into train and test data\n",
        "train_french_input, test_french_input, train_english_output, test_english_output = train_test_split(french_sequences, \n",
        "                                                    english_sequences, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=1)\n",
        "\n",
        "num_train_samples = train_french_input.shape[0]\n",
        "num_test_samples = test_french_input.shape[0]\n",
        "print(f'Number of training samples: {num_train_samples}')\n",
        "print(f'Number of testing samples:  {num_test_samples}')\n",
        "print()\n",
        "\n",
        "max_english_sentence_length = train_french_input.shape[1]\n",
        "max_french_sentence_length = train_french_input.shape[1]\n",
        "print(f'Max english sentence length:    {max_english_sentence_length}')\n",
        "print(f'Max french sentence length:     {max_french_sentence_length}')\n",
        "print()\n",
        "\n",
        "train_french_input = train_french_input.reshape(num_train_samples, max_french_sentence_length, 1)\n",
        "train_english_output = pad_sequences(train_english_output, maxlen=max_french_sentence_length, padding='post')\n",
        "train_english_output = train_english_output.reshape(num_train_samples, max_french_sentence_length, 1)\n",
        "\n",
        "test_french_input = test_french_input.reshape(num_test_samples, max_french_sentence_length, 1)\n",
        "test_english_output = pad_sequences(test_english_output, maxlen=max_french_sentence_length, padding='post')\n",
        "test_english_output = test_english_output.reshape(num_test_samples, max_french_sentence_length, 1)\n",
        "\n",
        "print(f'Train French:   {train_french_input.shape}')\n",
        "print(f'Test French:    {test_french_input.shape}')\n",
        "print(f'Train English:  {train_english_output.shape}')\n",
        "print(f'Test English:   {test_english_output.shape}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbqlYKQbTG3C",
        "colab_type": "text"
      },
      "source": [
        "## What are Recurrent Neural Networks?\n",
        "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed graph along a sequence. This allows it to exhibit dynamic temporal behavior for a time sequence. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
        "\n",
        "\n",
        "RNNs are designed to take sequences of text as inputs or return sequences of text as outputs, or both. \n",
        "\n",
        "They're called recurrent because the network's hidden layers have a loop in which the output from one time step becomes an input at the next time step. This recurrence serves as a form of memory. \n",
        "\n",
        "It allows contextual information to flow through the network so that relevant outputs from previous time steps can be applied to network operations at the current time step. \n",
        "\n",
        "<img src=\"https://qph.fs.quoracdn.net/main-qimg-6eced51767f5bcd94b32bbe50da438e9\">\n",
        "\n",
        "# **Vanishing Gradient Problem **\n",
        "\n",
        "As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train.\n",
        "\n",
        "A large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small.\n",
        "\n",
        "A small gradient means that the weights and biases of the initial layers will not be updated effectively with each training session. Since these initial layers are often crucial to recognizing the core elements of the input data, it can lead to overall inaccuracy of the whole network.\n",
        "\n",
        "\n",
        "\n",
        "## What are LSTMs (Long short-term memory)?\n",
        "\n",
        "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
        "\n",
        "\n",
        "A common LSTM unit is composed of a **cell**, an **input gate**, an **output gate** and a **forget gate**. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
        "\n",
        "RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged.\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png'>\n",
        "\n",
        "\n",
        "\n",
        "**The cell** : Responsible for keeping track of the dependencies between the elements in the input sequence. \n",
        "\n",
        "**The input gate** : Controls the extent to which a new value flows into the cell.\n",
        "\n",
        "**The forget gate**: Controls the extent to which a value remains in the cell and the output gate controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit. \n",
        "\n",
        "The activation function of the LSTM gates is often the logistic sigmoid function.\n",
        "\n",
        "<img src='https://miro.medium.com/max/2840/1*0f8r3Vd-i4ueYND1CUrhMA.png'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfzeifs8xgRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "#Create and Build the RNN model\n",
        "model = Sequential()\n",
        "#model.add(Embedding(input_dim=len(fren_tokenizer.word_index) + 1, output_dim=128, mask_zero=True))\n",
        "\n",
        "\n",
        "# return sequences is to get the output of the LSTM for each time step to pass\n",
        "#   to the next layer in the model\n",
        "model.add(LSTM(256, input_shape=train_french_input.shape[1:], return_sequences=True)) # Layer 1 (Input Layer)\n",
        "\n",
        "model.add(TimeDistributed(Dense(512, activation='tanh'))) # Layer 2 (Only hidden layer)\n",
        "\n",
        "# model ouput probabilities for english words from input word\n",
        "model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax'))) # Final (Output) Layer\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12pnwzua0QLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run the model on training data\n",
        "\n",
        "model.fit(train_french_input, train_english_output, batch_size=1024, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3tl93igTzCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('50_epochs_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV5tlM7O_ITs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load data\n",
        "model = load_model('50_epochs_model.h5')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlELtfDwoX4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict on unseen data\n",
        "sen_prediction = model.predict_classes(test_french_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAAkxhlunQ_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "          if index == n:\n",
        "              return word\n",
        "      return None\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePvVwom4n8TI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_text=[]\n",
        "for i in sen_prediction:\n",
        "    temp = []\n",
        "    for j in range(len(i)):\n",
        "      t = get_word(i[j], eng_tokenizer)\n",
        "      if j > 0:\n",
        "        if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "          temp.append('')\n",
        "        else:\n",
        "          temp.append(t)\n",
        "      else:\n",
        "        if(t == None):\n",
        "          temp.append('')\n",
        "        else:\n",
        "          temp.append(t)\n",
        "\n",
        "    preds_text.append(' '.join(temp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-ndicaCJJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Original Output and Predictions \n",
        "\n",
        "print(f\"Original Sentence:     {' '.join(fren_tokenizer.sequences_to_texts(test_french_input[50]))}\")\n",
        "\n",
        "\n",
        "print(f\"Expected Sentence:     {' '.join(eng_tokenizer.sequences_to_texts(test_english_output[50]))}\")\n",
        "\n",
        "print(\"Predicted Sentence:   \",preds_text[50])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkiupTQc40tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(fren_tokenizer.sequences_to_texts(test_french_input[2]))\n",
        "print(eng_tokenizer.sequences_to_texts(test_english_output[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcdFQMBZ4rXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(eng_tokenizer.word_index['autumn'])\n",
        "print(eng_tokenizer.word_index['fall'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}